{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import fetchData\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'CWAM'\n",
    "dataset_purposes = ['train', 'test']\n",
    "\n",
    "path_levels = ['part_1_', 'part_2_', 'part_3_', 'part_4_', 'part_5_', 'part_6_', 'part_7_', \n",
    "               'part_8_', 'part_9_', 'part_10_', 'part_11_', 'part_12_']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_folder_path(dataset_purpose):\n",
    "    folder_paths = []\n",
    "\n",
    "    base_dir = fetchData.get_defult_base_dir()\n",
    "    path = os.path.join(base_dir, data_type, dataset_purpose)\n",
    "    all_folders = os.listdir(path)\n",
    "\n",
    "    for prefix in path_levels:\n",
    "        matching_folders = [folder for folder in all_folders if folder.startswith(prefix)]   \n",
    "        if matching_folders:\n",
    "            folder_paths.append(matching_folders[0])\n",
    "    return folder_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_star_and_end_timestamp(part_path):\n",
    "    pattern = re.compile(r\"part_(\\d+)\"\n",
    "                     r\"_(?P<start_year>\\d{2})(?P<start_month>\\d{2})(?P<start_day>\\d{2})\"\n",
    "                     r\"_(?P<end_year>\\d{2})(?P<end_month>\\d{2})(?P<end_day>\\d{2})\")\n",
    "    match = pattern.match(part_path)\n",
    "    if match:\n",
    "        start_year = match.group(\"start_year\")\n",
    "        start_year = '20' + start_year\n",
    "        start_month = match.group(\"start_month\")\n",
    "        start_day = match.group(\"start_day\")\n",
    "        start_time = Timestamp.Timestamp(year=start_year, month=start_month, day=start_day)\n",
    "\n",
    "        end_year = match.group(\"end_year\")\n",
    "        end_year = '20' + end_year\n",
    "        end_month = match.group(\"end_month\")\n",
    "        end_day = match.group(\"end_day\")\n",
    "        end_time = Timestamp.Timestamp(year=end_year, month=end_month, day=end_day)\n",
    "    return [start_time, end_time]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_range(timerangelist):\n",
    "    start_date = timerangelist[0].to_string(\"%Y-%m-%d\")\n",
    "    end_date = timerangelist[1].to_string(\"%Y-%m-%d\")\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "    \n",
    "    timestamp_list = [\n",
    "        Timestamp.Timestamp(year=date.year, month=date.month, day=date.day) for date in date_range\n",
    "    ]\n",
    "\n",
    "    return timestamp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_forecast_files(directory, date):\n",
    "    year = date.to_string(\"%Y\")\n",
    "    month = date.to_string(\"%m\")\n",
    "    day = date.to_string(\"%d\")\n",
    "\n",
    "    pattern = os.path.join(directory, f\"{year}_{month}_{day}_??_??_GMT.Forecast.h5.CWAM.h5\")\n",
    "    return glob.glob(pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_hour_minute(file_name, date):\n",
    "    pattern = re.compile(r\"(\\d{4})\"\n",
    "                     r\"_(\\d{2})\"\n",
    "                     r\"_(\\d{2})\"\n",
    "                     r\"_(?P<hour>\\d{2})\"\n",
    "                     r\"_(?P<minute>\\d{2})\"\n",
    "                     r\"_GMT.Forecast\")\n",
    "    match = pattern.match(file_name)\n",
    "    if match:\n",
    "        date.hour = int(match.group(\"hour\"))\n",
    "        date.minute = int(match.group(\"minute\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas_timestamp(timestamp):\n",
    "    \n",
    "    local_time = pd.Timestamp(\n",
    "        year=timestamp.year,\n",
    "        month=timestamp.month,\n",
    "        day=timestamp.day,\n",
    "        hour=timestamp.hour if timestamp.hour is not None else 0,\n",
    "        minute=timestamp.minute if timestamp.minute is not None else 0\n",
    "    )\n",
    "    return local_time.tz_localize('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_cwam_data(data_df, base_time):\n",
    "    base_time = to_pandas_timestamp(base_time)\n",
    "\n",
    "    data_df[\"Forecast Time (FCST)\"] = data_df[\"Forecast Time (FCST)\"].str.replace('FCST', '').astype(int)\n",
    "    data_df[\"Forecast Time (FCST)\"] = pd.to_timedelta(data_df[\"Forecast Time (FCST)\"], unit='m')\n",
    "    data_df[\"Actual Time\"] = base_time + data_df[\"Forecast Time (FCST)\"]\n",
    "    \n",
    "    data_df[\"Threshold (TRSH)\"] = data_df[\"Threshold (TRSH)\"].str.extract(r'(\\d+)').astype(float)\n",
    "    \n",
    "    data_df.rename(columns={'Actual Time': 'timestamp_15mins'}, inplace=True)\n",
    "\n",
    "    def mean_of_lists(series):\n",
    "        all_values = np.concatenate(series.values)\n",
    "        return np.mean(all_values)\n",
    "\n",
    "    data_df.set_index(\"timestamp_15mins\", inplace=True)\n",
    "    \n",
    "    aggregated_df = data_df.groupby(pd.Grouper(freq=\"15min\")).agg({\n",
    "        \"Threshold (TRSH)\": \"mean\",  \n",
    "        \"Latitudes\": mean_of_lists,         \n",
    "        \"Longitudes\": mean_of_lists\n",
    "    }).reset_index() \n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_df(file_data):\n",
    "    combined_df = pd.concat(file_data, ignore_index=True)\n",
    "\n",
    "    combined_df[\"Threshold (TRSH)\"] = pd.to_numeric(combined_df[\"Threshold (TRSH)\"], errors='coerce')\n",
    "    combined_df[\"Latitudes\"] = pd.to_numeric(combined_df[\"Latitudes\"], errors='coerce')\n",
    "    combined_df[\"Longitudes\"] = pd.to_numeric(combined_df[\"Longitudes\"], errors='coerce')\n",
    "    \n",
    "    aggregated_df = combined_df.groupby(\"timestamp_15mins\").agg({\n",
    "        \"Threshold (TRSH)\": \"mean\",    \n",
    "        \"Latitudes\": \"mean\",            \n",
    "        \"Longitudes\": \"mean\"            \n",
    "    }).reset_index()\n",
    "\n",
    "    return aggregated_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/CWAM/train/2022-09-01.csv saved!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m file_name \u001b[38;5;241m=\u001b[39m file_name[:\u001b[38;5;28mlen\u001b[39m(file_name)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m11\u001b[39m]\n\u001b[1;32m     28\u001b[0m set_hour_minute(file_name, date)\n\u001b[0;32m---> 29\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mfetchData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_purpose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_purpose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpart_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mday\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mday\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_name\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m df \u001b[38;5;241m=\u001b[39m aggregate_cwam_data(df, date)\n\u001b[1;32m     38\u001b[0m df_aligned \u001b[38;5;241m=\u001b[39m complete_df\u001b[38;5;241m.\u001b[39mmerge(df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp_15mins\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/finalProject/cs523Final/fetchData.py:242\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(data_type, dataset_purpose, path_level, month, day, file_name, base_dir)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m data_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCWAM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:  \u001b[38;5;66;03m# Load HDF5 data\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported file type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_ext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/finalProject/cs523Final/fetchData.py:91\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m polygon \u001b[38;5;129;01min\u001b[39;00m f[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeviation Probability/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforecast_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflight_level\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontour\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Construct the full path for the dataset\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDeviation Probability/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mforecast_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflight_level\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontour\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolygon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 91\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     92\u001b[0m     latitudes, longitudes \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m], dataset[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     94\u001b[0m     fcst \u001b[38;5;241m=\u001b[39m forecast_time\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/myenv/lib/python3.12/site-packages/h5py/_hl/dataset.py:781\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 781\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fast_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dataset_purpose in dataset_purposes:\n",
    "    folder_path_levels = get_full_folder_path(dataset_purpose)\n",
    "    for part_path in folder_path_levels:\n",
    "        timerangelist = get_star_and_end_timestamp(part_path)\n",
    "        timestamp_list = get_date_range(timerangelist)\n",
    "        for date in timestamp_list:\n",
    "            month = date.to_string(\"%m\")\n",
    "            day = date.to_string(\"%d\")\n",
    "            directory = os.path.join(fetchData.get_defult_base_dir(), \n",
    "                                     data_type, \n",
    "                                     dataset_purpose, \n",
    "                                     part_path, \n",
    "                                     month, \n",
    "                                     day\n",
    "                                    )\n",
    "            df_list = []\n",
    "            data_range = date.to_string(\"%Y-%m-%d\")\n",
    "\n",
    "            file_path = os.path.join('./', 'data', data_type, dataset_purpose, f'{data_range}.csv')\n",
    "            if os.path.exists(file_path):\n",
    "                print(file_path, \"saved!\")\n",
    "                continue\n",
    "            \n",
    "            start_date = pd.to_datetime(data_range).tz_localize('UTC')\n",
    "            end_date = start_date + pd.Timedelta(days=1)\n",
    "            complete_times = pd.date_range(start=start_date, end=end_date, freq='15min')\n",
    "            complete_index = pd.MultiIndex.from_product([complete_times], names=['timestamp_15mins'])\n",
    "            complete_df = pd.DataFrame(index=complete_index).reset_index()\n",
    "\n",
    "            forecast_files = find_forecast_files(directory, date)\n",
    "            for forecast_file in forecast_files:\n",
    "                file_name = forecast_file.split(os.path.sep)[-1]\n",
    "                file_name = file_name[:len(file_name)-11]\n",
    "                set_hour_minute(file_name, date)\n",
    "                df = fetchData.load_data( \n",
    "                    data_type=data_type, \n",
    "                    dataset_purpose=dataset_purpose, \n",
    "                    path_level=part_path, \n",
    "                    month=month, \n",
    "                    day=day, \n",
    "                    file_name=file_name\n",
    "                )\n",
    "                df = aggregate_cwam_data(df, date)\n",
    "                df_aligned = complete_df.merge(df, on='timestamp_15mins', how='left')\n",
    "                df_list.append(df_aligned)\n",
    "            final_df = get_final_df(df_list)\n",
    "            final_df = final_df.ffill().bfill()\n",
    "            final_df = final_df.interpolate(method='linear')\n",
    "\n",
    "            file_path = os.path.join('./', 'data', data_type, dataset_purpose, f'{data_range}.csv')\n",
    "            os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "            final_df.to_csv(file_path, index=False)\n",
    "            print(file_path, \"saved!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
