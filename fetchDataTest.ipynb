{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse METAR report \n",
    "def parse_metar_line(date_time, line):\n",
    "    \"\"\"Parse a single METAR report line into its components, including optional AUTO.\"\"\"\n",
    "    metar_pattern = re.compile(\n",
    "        r'^(?P<station>[A-Z]{4})\\s+'\n",
    "        r'(?P<datetime>\\d{2}\\d{4}Z)\\s+'\n",
    "        r'(?P<auto>AUTO\\s+)?'\n",
    "        r'(?P<wind>\\d{3}\\d{2}(G\\d{2})?(KT|MPS|KMH))?\\s*'\n",
    "        r'(?P<visibility>\\d{4}(SM|NDV)?|CAVOK|////)?\\s*'\n",
    "        r'(?P<clouds>(///\\d{3}///|FEW|SCT|BKN|OVC|NSC|VV|///)\\d{0,3}[A-Z]{0,3}\\s*)*'\n",
    "        r'(?P<temperature>M?\\d{2})/(?P<dewpoint>M?\\d{2}|//)\\s+'\n",
    "        r'Q(?P<pressure>\\d{4})\\s*'\n",
    "        r'(?P<remarks>.+)?$'\n",
    "    )\n",
    "    \n",
    "    match = metar_pattern.match(line)\n",
    "    if match:\n",
    "        data = match.groupdict()\n",
    "        \n",
    "        # Process the clouds group to handle multiple cloud layers\n",
    "        clouds = data.get('clouds')\n",
    "        if clouds:\n",
    "            data['clouds'] = clouds.strip().split()\n",
    "        else:\n",
    "            data['clouds'] = None\n",
    "        \n",
    "        data['date_time'] = date_time\n",
    "        # Handle optional fields\n",
    "        data['auto'] = data['auto'].strip() if data['auto'] else None\n",
    "        data['remarks'] = data['remarks'].strip() if data['remarks'] else None\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Parse TAF report \n",
    "def parse_taf_block(block):\n",
    "    \"\"\"Parse a full TAF block into its components.\"\"\"\n",
    "    # Regex pattern to match main TAF components across multiple lines\n",
    "    taf_pattern = re.compile(\n",
    "        r'(?P<station>TAF \\w{4})\\s+'                     # Station identifier (TAF + ICAO code)\n",
    "        r'(?P<datetime>\\d{6}Z)\\s+'                       # Date and time of issuance\n",
    "        r'(?P<validity>\\d{4}/\\d{4})\\s+'                  # Validity period\n",
    "        r'(?P<wind>(\\d{5}(KT|MPS|KMH)|VRB\\d{2}(KT|MPS|KMH)?)?)\\s*'  # Wind information\n",
    "        r'(?P<visibility>CAVOK|\\d{4})?\\s*'               # Visibility, including CAVOK\n",
    "        r'(?P<clouds>(SCT|BKN|FEW|NSC|OVC|NSC|VV)\\d{3}[A-Z]{0,3})?\\s*' # Cloud information\n",
    "        r'(?P<temp_dewpoint>(TX|TN)\\d{2}/\\d{4}Z)?\\s*'    # Max/min temperature forecast\n",
    "        \n",
    "        # Additional forecasts as separate groups\n",
    "        r'(?P<probability>(PROB\\d{2}\\s+\\d{4}/\\d{4}\\s+.*)*)'     # Probability forecasts (e.g., PROB30)\n",
    "        r'(?P<temporary>(TEMPO\\s+\\d{4}/\\d{4}\\s+.*)*)'        # Temporary forecasts\n",
    "        r'(?P<becoming>(BECMG\\s+\\d{4}/\\d{4}\\s+.*)*)'        # Becoming forecasts\n",
    "        r'(?P<from_forecasts>(FM\\d{6}\\s+.*)*)'                       # From forecasts\n",
    "    )\n",
    "    \n",
    "    match = taf_pattern.search(block)\n",
    "    if match:\n",
    "        return match.groupdict()\n",
    "    return None\n",
    "\n",
    "# CWAM\n",
    "def get_dataset(f):\n",
    "    data_entries = []\n",
    "    for forecast_time in f['Deviation Probability']:\n",
    "        for flight_level in f[f'Deviation Probability/{forecast_time}']:\n",
    "            for contour in f[f'Deviation Probability/{forecast_time}/{flight_level}']:\n",
    "                for threshold in f[f'Deviation Probability/{forecast_time}/{flight_level}/{contour}']:\n",
    "                    for polygon in f[f'Deviation Probability/{forecast_time}/{flight_level}/{contour}/{threshold}']:\n",
    "                        # Construct the full path for the dataset\n",
    "                        dataset_name = f'Deviation Probability/{forecast_time}/{flight_level}/{contour}/{threshold}/{polygon}'\n",
    "                        \n",
    "                        dataset = f[dataset_name][:]\n",
    "                        latitudes, longitudes = dataset[0], dataset[1]\n",
    "                        \n",
    "                        fcst = forecast_time\n",
    "                        flvl = flight_level\n",
    "                        trsh = threshold\n",
    "                        poly = polygon\n",
    "\n",
    "                        data_entries.append({\n",
    "                            \"Forecast Time (FCST)\" : fcst,\n",
    "                            \"Flight Level (FLVL)\" : flvl,\n",
    "                            \"Threshold (TRSH)\" : trsh,\n",
    "                            \"Polygon Number (POLY)\" : poly,\n",
    "                            \"Latitudes\" : latitudes,\n",
    "                            \"Longitudes\" : longitudes\n",
    "                        })\n",
    "    return pd.DataFrame(data_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_type, dataset_purpose, path_level=None, month=None, day=None, file_name=None, base_dir=None):\n",
    "    \"\"\"\n",
    "    Loads a data file from a specified path.\n",
    "    \n",
    "    Parameters\n",
    "    - data_type (str): Data type, e.g., \"CWAM\", \"FUSER\", \"METAR\", \"TAF\".\n",
    "    - dataset_purpose (str): \"train\" or \"test\", indicating training or testing data.\n",
    "    - path_level (str): CWAM or METAR part_X, e.g., \"part_1\".\n",
    "    - month (str): Month, e.g., \"09\".\n",
    "    - day (str): Day, e.g., \"29\".\n",
    "    - file_name (str): Filename (without extension or type-specific suffix), e.g., \"2022_09_29_20_00_GMT.Forecast\".\n",
    "    - base_dir (str): Root directory of the data, e.g., \"/home/finalProject/data\".\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame, HDF5 file object, or string content based on file type.\n",
    "    \"\"\"\n",
    "\n",
    "    base_dir = \"/home/jaosn/finalProject/data\" if not base_dir else base_dir\n",
    "    # Specify file extensions\n",
    "    file_ext = file_extension[data_type]\n",
    "    \n",
    "    # Build the file path based on data type\n",
    "    if data_type == \"CWAM\":\n",
    "        # CWAM Path: data/CWAM/test/part_X/MM/DD\n",
    "        if not (path_level and month and day):\n",
    "            raise ValueError(\"CWAM data requires path_level, month, and day\")\n",
    "        path = os.path.join(base_dir, data_type, dataset_purpose, path_level, month, day)\n",
    "        # CWAM files use a specific naming convention\n",
    "        file_path = os.path.join(path, f\"{file_name}.h5.CWAM.h5\")\n",
    "        \n",
    "    elif data_type == \"FUSER\":\n",
    "        # FUSER Path: data/FUSER/train/KXXX/\n",
    "        if not path_level:\n",
    "            raise ValueError(\"FUSER data requires fuser_type\")\n",
    "        path = os.path.join(base_dir, data_type, dataset_purpose, path_level)\n",
    "        file_path = os.path.join(path, f\"{file_name}.csv\")\n",
    "    \n",
    "    elif data_type in [\"METAR\", \"TAF\"]:\n",
    "        # METAR and TAF Paths: data/METAR/train/part_X/\n",
    "        if dataset_purpose == \"train\" and not path_level:\n",
    "            raise ValueError(f\"{data_type} data requires path_level\")\n",
    "        if dataset_purpose == \"train\":\n",
    "            path = os.path.join(base_dir, data_type, dataset_purpose, path_level)\n",
    "        else:\n",
    "            path = os.path.join(base_dir, data_type, dataset_purpose)\n",
    "        file_path = os.path.join(path, f\"{file_name}.txt\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported data type: {data_type}\")\n",
    "\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    \n",
    "    # Load file based on file type\n",
    "    if data_type == \"FUSER\":\n",
    "        def extract_file_type(file_name):\n",
    "            pattern = r'^(?P<airport>\\w+)' \\\n",
    "                      r'_(?P<date_range>\\d{4}-\\d{2}-\\d{2}(_\\d{4}-\\d{2}-\\d{2})?)' \\\n",
    "                      r'\\.(?P<file_type>\\w+)_data_set$'\n",
    "              \n",
    "            match = re.match(pattern, file_name)\n",
    "            \n",
    "            if match:\n",
    "                file_type = match.group('file_type')\n",
    "                return file_type\n",
    "            else:\n",
    "                return None\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        file_type = extract_file_type(file_name)\n",
    "        df['file_type'] = file_type\n",
    "        return df\n",
    "    elif data_type in [\"METAR\", \"TAF\"]:\n",
    "        if data_type == \"METAR\":\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                lines = [line.strip() for line in lines if line.strip()]\n",
    "                data_entries = []\n",
    "                date_time = None  # To keep track of the current date and time\n",
    "\n",
    "                for line in lines:\n",
    "                    # Check if the line is a date line\n",
    "                    if re.match(r'\\d{4}/\\d{2}/\\d{2} \\d{2}:\\d{2}', line):\n",
    "                        date_time = line  # Store the date and time\n",
    "                    elif date_time:  # If we have a date_time, process the METAR line\n",
    "                        parsed_data = parse_metar_line(date_time, line)\n",
    "                        if parsed_data:\n",
    "                            data_entries.append(parsed_data)\n",
    "                        date_time = None \n",
    "            return pd.DataFrame(data_entries)\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                lines = file.readlines()\n",
    "    \n",
    "            data_entries = []\n",
    "            buffer = \"\"  # To accumulate lines for a single TAF block\n",
    "\n",
    "            i = 0\n",
    "            while i < len(lines):\n",
    "                line = lines[i].strip()\n",
    "                \n",
    "                # If the line starts with a date, treat it as a new TAF report block\n",
    "                date_match = re.match(r'\\d{4}/\\d{2}/\\d{2} \\d{2}:\\d{2}', line)\n",
    "                if date_match:\n",
    "                    if buffer:  # Parse the previous buffer before moving to the next date block\n",
    "                        parsed_data = parse_taf_block(buffer)\n",
    "                        if parsed_data:\n",
    "                            parsed_data[\"Date and Time\"] = date_time  # Associate with the last stored date\n",
    "                            data_entries.append(parsed_data)\n",
    "                        buffer = \"\"  # Clear buffer for the new TAF block\n",
    "                    date_time = date_match.group()  # Update date_time to the new block's date\n",
    "\n",
    "                # Continue adding lines to the buffer for the current TAF block\n",
    "                if line.startswith(\"TAF\") or buffer:\n",
    "                    buffer += line + \" \"\n",
    "\n",
    "                # Move to the next line\n",
    "                i += 1\n",
    "\n",
    "            # Parse the last block in the buffer after the loop ends\n",
    "            if buffer:\n",
    "                parsed_data = parse_taf_block(buffer)\n",
    "                if parsed_data:\n",
    "                    parsed_data[\"Date and Time\"] = date_time\n",
    "                    data_entries.append(parsed_data)\n",
    "\n",
    "            # Convert the list of dictionaries to a DataFrame\n",
    "            return pd.DataFrame(data_entries)\n",
    "    elif data_type == \"CWAM\":\n",
    "        with h5py.File(file_path, 'r') as file:  # Load HDF5 data\n",
    "            \n",
    "            return get_dataset(file)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_ext}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fetchData\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwam_data = fetchData.load_data( \n",
    "    data_type=\"CWAM\", \n",
    "    dataset_purpose=\"train\", \n",
    "    path_level=\"part_1_220901_220924\", \n",
    "    month=\"09\", \n",
    "    day=\"01\", \n",
    "    file_name=\"2022_09_01_20_00_GMT.Forecast\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Forecast Time (FCST) Flight Level (FLVL) Threshold (TRSH)  \\\n",
      "0                   FCST000             FLVL250          TRSH060   \n",
      "1                   FCST000             FLVL250          TRSH060   \n",
      "2                   FCST000             FLVL250          TRSH060   \n",
      "3                   FCST000             FLVL250          TRSH060   \n",
      "4                   FCST000             FLVL250          TRSH060   \n",
      "...                     ...                 ...              ...   \n",
      "104484              FCST120             FLVL450          TRSH080   \n",
      "104485              FCST120             FLVL450          TRSH080   \n",
      "104486              FCST120             FLVL450          TRSH080   \n",
      "104487              FCST120             FLVL450          TRSH080   \n",
      "104488              FCST120             FLVL450          TRSH080   \n",
      "\n",
      "       Polygon Number (POLY)  \\\n",
      "0                   POLY0001   \n",
      "1                   POLY0002   \n",
      "2                   POLY0003   \n",
      "3                   POLY0004   \n",
      "4                   POLY0005   \n",
      "...                      ...   \n",
      "104484              POLY0007   \n",
      "104485              POLY0008   \n",
      "104486              POLY0009   \n",
      "104487              POLY0010   \n",
      "104488              POLY0011   \n",
      "\n",
      "                                                Latitudes  \\\n",
      "0       [22.47349, 22.461931, 22.456131, 22.450317, 22...   \n",
      "1       [23.82428, 23.82375, 23.823462, 23.82316, 23.8...   \n",
      "2       [23.934841, 23.935038, 23.935114, 23.935177, 2...   \n",
      "3       [22.827188, 22.815912, 22.810253, 22.80458, 22...   \n",
      "4       [22.66365, 22.651442, 22.645317, 22.63918, 22....   \n",
      "...                                                   ...   \n",
      "104484  [32.016056, 32.00316, 31.996683, 31.990194, 32...   \n",
      "104485  [32.15372, 32.140953, 32.169758, 32.24018, 32....   \n",
      "104486  [33.5222, 33.518787, 33.51705, 33.515305, 33.5...   \n",
      "104487  [32.36117, 32.34764, 32.340855, 32.33405, 32.3...   \n",
      "104488  [33.297306, 33.28531, 33.279285, 33.27325, 33....   \n",
      "\n",
      "                                               Longitudes  \n",
      "0       [-81.81963, -81.7427, -81.70423, -81.66577, -8...  \n",
      "1       [-97.29758, -97.21953, -97.180504, -97.14149, ...  \n",
      "2       [-98.31249, -98.23437, -98.195305, -98.15624, ...  \n",
      "3       [-82.24263, -82.16547, -82.126884, -82.08831, ...  \n",
      "4       [-80.92684, -80.84982, -80.81131, -80.772804, ...  \n",
      "...                                                   ...  \n",
      "104484  [-80.79678, -80.712685, -80.67064, -80.6286, -...  \n",
      "104485  [-80.98617, -80.901924, -80.85276, -80.83862, ...  \n",
      "104486  [-93.51351, -93.42732, -93.38424, -93.34115, -...  \n",
      "104487  [-79.98647, -79.902115, -79.85995, -79.81778, ...  \n",
      "104488  [-82.11523, -82.029816, -81.987114, -81.94442,...  \n",
      "\n",
      "[104489 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/CWAM/CWAM_example.csv saved!\n"
     ]
    }
   ],
   "source": [
    "print(cwam_data)\n",
    "file_path = os.path.join('./', 'data', 'CWAM', f'CWAM_example.csv')\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "cwam_data.to_csv(file_path, index=False)\n",
    "print(file_path, \"saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuser_data = fetchData.load_data(\n",
    "    data_type=\"FUSER\", \n",
    "    dataset_purpose=\"train\", \n",
    "    path_level=\"KATL\", \n",
    "    file_name=\"KATL_2022-09-01.configs_data_set\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   airport_id        data_header src_addr           datis_time  \\\n",
      "0        KATL  AD ATL /OS DT0052  ATLATXA  2022-09-01 00:52:00   \n",
      "1        KATL  AD ATL /OS DX0152  ATLATXA  2022-09-01 01:53:00   \n",
      "2        KATL  AD ATL /OS DY0152  ATLATXA  2022-09-01 02:06:00   \n",
      "3        KATL  AD ATL /OS DZ0252  ATLATXA  2022-09-01 02:54:00   \n",
      "4        KATL  AD ATL /OS DN0252  ATLATXA  2022-09-01 03:32:00   \n",
      "5        KATL  AD ATL /OS DO0352  ATLATXA  2022-09-01 04:04:00   \n",
      "6        KATL  AD ATL /OS DP0352  ATLATXA  2022-09-01 04:06:00   \n",
      "7        KATL  AD ATL /OS DQ0452  ATLATXA  2022-09-01 04:54:00   \n",
      "8        KATL  AD ATL /OS DR0552  ATLATXA  2022-09-01 06:06:00   \n",
      "9        KATL  AD ATL /OS DS0652  ATLATXA  2022-09-01 06:53:00   \n",
      "10       KATL  AD ATL /OS DT0752  ATLATXA  2022-09-01 07:56:00   \n",
      "11       KATL  AD ATL /OS DU0752  ATLATXA  2022-09-01 08:15:00   \n",
      "12       KATL  AD ATL /OS DV0852  ATLATXA  2022-09-01 09:08:00   \n",
      "13       KATL  AD ATL /OS DW0852  ATLATXA  2022-09-01 09:38:00   \n",
      "14       KATL  AD ATL /OS DX0852  ATLATXA  2022-09-01 09:43:00   \n",
      "15       KATL  AD ATL /OS DY0952  ATLATXA  2022-09-01 09:54:00   \n",
      "16       KATL  AD ATL /OS DZ0952  ATLATXA  2022-09-01 10:03:00   \n",
      "17       KATL  AD ATL /OS DN0952  ATLATXA  2022-09-01 10:18:00   \n",
      "18       KATL  AD ATL /OS DO1052  ATLATXA  2022-09-01 10:55:00   \n",
      "19       KATL  AD ATL /OS DP1052  ATLATXA  2022-09-01 11:48:00   \n",
      "20       KATL  AD ATL /OS DQ1152  ATLATXA  2022-09-01 11:58:00   \n",
      "21       KATL  AD ATL /OS DR1152  ATLATXA  2022-09-01 12:15:00   \n",
      "22       KATL  AD ATL /OS DS1252  ATLATXA  2022-09-01 12:53:00   \n",
      "23       KATL  AD ATL /OS DT1352  ATLATXA  2022-09-01 13:52:00   \n",
      "24       KATL  AD ATL /OS DU1452  ATLATXA  2022-09-01 14:54:00   \n",
      "25       KATL  AD ATL /OS DV1552  ATLATXA  2022-09-01 15:53:00   \n",
      "26       KATL  AD ATL /OS DW1652  ATLATXA  2022-09-01 16:53:00   \n",
      "27       KATL  AD ATL /OS DX1752  ATLATXA  2022-09-01 17:53:00   \n",
      "28       KATL  AD ATL /OS DN1852  ATLATXA  2022-09-01 18:53:00   \n",
      "29       KATL  AD ATL /OS DO1952  ATLATXA  2022-09-01 19:53:00   \n",
      "30       KATL  AD ATL /OS DP2052  ATLATXA  2022-09-01 20:52:00   \n",
      "31       KATL  AD ATL /OS DQ2152  ATLATXA  2022-09-01 21:54:00   \n",
      "32       KATL  AD ATL /OS DR2252  ATLATXA  2022-09-01 22:53:00   \n",
      "33       KATL  AD ATL /OS DS2352  ATLATXA  2022-09-01 23:52:00   \n",
      "\n",
      "             start_time                                     weather_report  \\\n",
      "0   2022-09-01 00:52:00   30006KT 10SM FEW250 26/16 A2999 (TWO NINER NI...   \n",
      "1   2022-09-01 01:53:00   31009KT 10SM FEW250 25/16 A3001 (THREE ZERO Z...   \n",
      "2   2022-09-01 02:06:00   31009KT 10SM FEW250 25/16 A3001 (THREE ZERO Z...   \n",
      "3   2022-09-01 02:54:00   31006KT 10SM FEW060 FEW250 24/16 A3001 (THREE...   \n",
      "4   2022-09-01 03:32:00   31006KT 10SM FEW060 FEW250 24/16 A3001 (THREE...   \n",
      "5   2022-09-01 04:04:00   31005KT 10SM FEW060 FEW250 23/17 A3001 (THREE...   \n",
      "6   2022-09-01 04:06:00   31005KT 10SM FEW060 FEW250 23/17 A3001 (THREE...   \n",
      "7   2022-09-01 04:54:00   30004KT 10SM FEW060 23/17 A3000 (THREE ZERO Z...   \n",
      "8   2022-09-01 06:06:00   33004KT 10SM CLR 23/16 A3000 (THREE ZERO ZERO...   \n",
      "9   2022-09-01 06:53:00   00000KT 10SM CLR 22/17 A3000 (THREE ZERO ZERO...   \n",
      "10  2022-09-01 07:56:00   31003KT 10SM FEW049 22/17 A3001 (THREE ZERO Z...   \n",
      "11  2022-09-01 08:15:00   31003KT 10SM FEW049 22/17 A3001 (THREE ZERO Z...   \n",
      "12  2022-09-01 09:08:00   32004KT 10SM FEW050 21/17 A3001 (THREE ZERO Z...   \n",
      "13  2022-09-01 09:38:00   32004KT 10SM FEW050 21/17 A3001 (THREE ZERO Z...   \n",
      "14  2022-09-01 09:43:00   32004KT 10SM FEW050 21/17 A3001 (THREE ZERO Z...   \n",
      "15  2022-09-01 09:54:00   01006KT 10SM CLR 22/16 A3001 (THREE ZERO ZERO...   \n",
      "16  2022-09-01 10:03:00   01006KT 10SM CLR 22/16 A3001 (THREE ZERO ZERO...   \n",
      "17  2022-09-01 10:18:00   01006KT 10SM CLR 22/16 A3001 (THREE ZERO ZERO...   \n",
      "18  2022-09-01 10:55:00   35005KT 10SM FEW055 FEW080 21/16 A3002 (THREE...   \n",
      "19  2022-09-01 11:48:00   35005KT 10SM FEW055 FEW080 21/16 A3002 (THREE...   \n",
      "20  2022-09-01 11:58:00   04005KT 10SM FEW055 FEW080 FEW250 23/16 A3003...   \n",
      "21  2022-09-01 12:15:00   04005KT 10SM FEW055 FEW080 FEW250 23/16 A3003...   \n",
      "22  2022-09-01 12:53:00   08008KT 10SM FEW065 FEW250 24/17 A3004 (THREE...   \n",
      "23  2022-09-01 13:52:00   08007KT 10SM FEW070 26/18 A3005 (THREE ZERO Z...   \n",
      "24  2022-09-01 14:54:00   12004KT 10SM FEW003 FEW075 FEW250 28/19 A3007...   \n",
      "25  2022-09-01 15:53:00   15008KT 10SM FEW040 FEW080 FEW250 30/19 A3007...   \n",
      "26  2022-09-01 16:53:00   08007KT 10SM FEW046 SCT080 SCT250 31/21 A3006...   \n",
      "27  2022-09-01 17:53:00   VRB03KT 10SM SCT050 BKN080 BKN250 32/19 A3004...   \n",
      "28  2022-09-01 18:53:00   00000KT 10SM SCT055 SCT080 SCT250 32/19 A3001...   \n",
      "29  2022-09-01 19:53:00   16005KT 10SM SCT055 SCT080 SCT200 32/19 A2999...   \n",
      "30  2022-09-01 20:52:00   21005KT 10SM FEW055TCU SCT080 SCT200 32/19 A2...   \n",
      "31  2022-09-01 21:54:00   19005KT 10SM SCT055TCU SCT080 BKN200 32/20 A2...   \n",
      "32  2022-09-01 22:53:00   10006KT 10SM FEW048TCU SCT070 SCT200 31/21 A2...   \n",
      "33  2022-09-01 23:52:00   15007KT 10SM FEW048TCU FEW200 28/22 A2999 (TW...   \n",
      "\n",
      "   departure_runways arrival_runways timestamp_source_received  \\\n",
      "0           26L, 27R    26R, 27L, 28       2022-09-01 00:52:51   \n",
      "1           26L, 27R    26R, 27L, 28       2022-09-01 01:53:12   \n",
      "2           26L, 27R    26R, 27L, 28       2022-09-01 02:06:05   \n",
      "3           26L, 27R    26R, 27L, 28       2022-09-01 02:54:44   \n",
      "4           26L, 27R        26R, 27R       2022-09-01 03:32:39   \n",
      "5           26L, 27R        26R, 27R       2022-09-01 04:04:59   \n",
      "6           26L, 27R        26R, 27R       2022-09-01 04:06:20   \n",
      "7           26L, 27R        26R, 27R       2022-09-01 04:54:30   \n",
      "8           26L, 27R        26R, 27R       2022-09-01 06:06:50   \n",
      "9           26L, 27R        26R, 27R       2022-09-01 06:53:05   \n",
      "10          26L, 27R        26R, 27R       2022-09-01 07:56:17   \n",
      "11          26L, 27R        26R, 27R       2022-09-01 08:15:53   \n",
      "12          26L, 27R        26R, 27R       2022-09-01 09:08:44   \n",
      "13          26L, 27R        26R, 27R       2022-09-01 09:38:21   \n",
      "14          26L, 27R    26R, 27L, 28       2022-09-01 09:43:51   \n",
      "15          26L, 27R    26R, 27L, 28       2022-09-01 09:54:52   \n",
      "16          26L, 27R    26R, 27L, 28       2022-09-01 10:03:39   \n",
      "17            8R, 9L      10, 8L, 9R       2022-09-01 10:18:30   \n",
      "18            8R, 9L      10, 8L, 9R       2022-09-01 10:55:26   \n",
      "19            8R, 9L      10, 8L, 9R       2022-09-01 11:48:13   \n",
      "20            8R, 9L      10, 8L, 9R       2022-09-01 11:58:11   \n",
      "21            8R, 9L      10, 8L, 9R       2022-09-01 12:15:47   \n",
      "22            8R, 9L      10, 8L, 9R       2022-09-01 12:53:12   \n",
      "23            8R, 9L      10, 8L, 9R       2022-09-01 13:52:57   \n",
      "24            8R, 9L      10, 8L, 9R       2022-09-01 14:54:27   \n",
      "25            8R, 9L      10, 8L, 9R       2022-09-01 15:53:03   \n",
      "26            8R, 9L      10, 8L, 9R       2022-09-01 16:53:25   \n",
      "27            8R, 9L      10, 8L, 9R       2022-09-01 17:53:45   \n",
      "28            8R, 9L      10, 8L, 9R       2022-09-01 18:53:11   \n",
      "29            8R, 9L      10, 8L, 9R       2022-09-01 19:53:19   \n",
      "30            8R, 9L      10, 8L, 9R       2022-09-01 20:52:51   \n",
      "31            8R, 9L      10, 8L, 9R       2022-09-01 21:54:45   \n",
      "32            8R, 9L      10, 8L, 9R       2022-09-01 22:53:10   \n",
      "33            8R, 9L      10, 8L, 9R       2022-09-01 23:52:53   \n",
      "\n",
      "   timestamp_source_processed  invalid_departure_runways  \\\n",
      "0         2022-09-01 00:52:51                        NaN   \n",
      "1         2022-09-01 01:53:12                        NaN   \n",
      "2         2022-09-01 02:06:05                        NaN   \n",
      "3         2022-09-01 02:54:44                        NaN   \n",
      "4         2022-09-01 03:32:39                        NaN   \n",
      "5         2022-09-01 04:04:59                        NaN   \n",
      "6         2022-09-01 04:06:20                        NaN   \n",
      "7         2022-09-01 04:54:30                        NaN   \n",
      "8         2022-09-01 06:06:50                        NaN   \n",
      "9         2022-09-01 06:53:05                        NaN   \n",
      "10        2022-09-01 07:56:17                        NaN   \n",
      "11        2022-09-01 08:15:53                        NaN   \n",
      "12        2022-09-01 09:08:44                        NaN   \n",
      "13        2022-09-01 09:38:21                        NaN   \n",
      "14        2022-09-01 09:43:51                        NaN   \n",
      "15        2022-09-01 09:54:52                        NaN   \n",
      "16        2022-09-01 10:03:39                        NaN   \n",
      "17        2022-09-01 10:18:30                        2.0   \n",
      "18        2022-09-01 10:55:26                        2.0   \n",
      "19        2022-09-01 11:48:13                        2.0   \n",
      "20        2022-09-01 11:58:11                        2.0   \n",
      "21        2022-09-01 12:15:47                        2.0   \n",
      "22        2022-09-01 12:53:12                        2.0   \n",
      "23        2022-09-01 13:52:57                        2.0   \n",
      "24        2022-09-01 14:54:27                        2.0   \n",
      "25        2022-09-01 15:53:03                        2.0   \n",
      "26        2022-09-01 16:53:25                        2.0   \n",
      "27        2022-09-01 17:53:45                        2.0   \n",
      "28        2022-09-01 18:53:11                        2.0   \n",
      "29        2022-09-01 19:53:19                        2.0   \n",
      "30        2022-09-01 20:52:51                        2.0   \n",
      "31        2022-09-01 21:54:45                        2.0   \n",
      "32        2022-09-01 22:53:10                        2.0   \n",
      "33        2022-09-01 23:52:53                        2.0   \n",
      "\n",
      "    invalid_arrival_runways  \\\n",
      "0                       NaN   \n",
      "1                       NaN   \n",
      "2                       NaN   \n",
      "3                       NaN   \n",
      "4                       NaN   \n",
      "5                       NaN   \n",
      "6                       NaN   \n",
      "7                       NaN   \n",
      "8                       NaN   \n",
      "9                       NaN   \n",
      "10                      NaN   \n",
      "11                      NaN   \n",
      "12                      NaN   \n",
      "13                      NaN   \n",
      "14                      NaN   \n",
      "15                      NaN   \n",
      "16                      NaN   \n",
      "17                      NaN   \n",
      "18                      NaN   \n",
      "19                      NaN   \n",
      "20                      NaN   \n",
      "21                      NaN   \n",
      "22                      NaN   \n",
      "23                      NaN   \n",
      "24                      NaN   \n",
      "25                      NaN   \n",
      "26                      NaN   \n",
      "27                      NaN   \n",
      "28                      NaN   \n",
      "29                      NaN   \n",
      "30                      NaN   \n",
      "31                      NaN   \n",
      "32                      NaN   \n",
      "33                      NaN   \n",
      "\n",
      "                              departure_runway_string  \\\n",
      "0   SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT RNAV OF...   \n",
      "1   SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT RNAV OF...   \n",
      "2   SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT RNAV OF...   \n",
      "3   SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT RNAV OF...   \n",
      "4   SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "5   SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "6   SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "7   SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "8   SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "9   SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "10  SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "11  SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "12  SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "13  SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "14  SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "15  SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "16  SIMUL DEPS, DEPG RWYS, 26L, 27R. XPECT INITIAL...   \n",
      "17  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT INTIAL HE...   \n",
      "18  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "19  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "20  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "21  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "22  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "23  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "24  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "25  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "26  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "27  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "28  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "29  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "30  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "31  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "32  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "33  SIMUL DEPS, DEPG RWYS, 8R, 9L. XPECT RNAV OFF ...   \n",
      "\n",
      "                                arrival_runway_string  \\\n",
      "0   SIMULTANEOUS APCHS IN USE VIS 26R, ILS 27L, VI...   \n",
      "1   SIMULTANEOUS APCHS IN USE VIS 26R, ILS 27L, VI...   \n",
      "2   SIMULTANEOUS APCHS IN USE VIS 26R, ILS 27L, VI...   \n",
      "3   SIMULTANEOUS APCHS IN USE VIS 26R, ILS 27L, VI...   \n",
      "4   VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 27R.   \n",
      "5   VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 27R.   \n",
      "6   VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 27R.   \n",
      "7   VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 27R.   \n",
      "8   VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 27R.   \n",
      "9   VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 27R.   \n",
      "10  VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 27R.   \n",
      "11  VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 27R.   \n",
      "12  VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 27R.   \n",
      "13  VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 27R.   \n",
      "14  VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 2...   \n",
      "15  VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 2...   \n",
      "16  VISUAL APPROACH RWY 26R, VISUAL APPROACH RWY 2...   \n",
      "17  SIMULTANEOUS APCHS IN USE VIS 8L, VIS 9R, VIS 10.   \n",
      "18  SIMULTANEOUS APCHS IN USE VIS 8L, VIS 9R, VIS 10.   \n",
      "19  SIMULTANEOUS APCHS IN USE VIS 8L, VIS 9R, VIS 10.   \n",
      "20  SIMULTANEOUS APCHS IN USE VIS 8L, VIS 9R, VIS 10.   \n",
      "21  SIMULTANEOUS APCHS IN USE VIS 8L, VIS 9R, VIS 10.   \n",
      "22  SIMULTANEOUS APCHS IN USE VIS 8L, VIS 9R, VIS 10.   \n",
      "23  SIMULTANEOUS APCHS IN USE VIS 8L, VIS 9R, VIS 10.   \n",
      "24  SIMULTANEOUS APCHS IN USE VIS 8L, VIS 9R, VIS 10.   \n",
      "25  SIMULTANEOUS APCHS IN USE VIS 8L, VIS 9R, VIS 10.   \n",
      "26  SIMULTANEOUS APCHS IN USE VIS 8L, VIS 9R, VIS 10.   \n",
      "27  SIMULTANEOUS APCHS IN USE VIS 8L, ILS 9R, VIS 10.   \n",
      "28  SIMULTANEOUS APCHS IN USE VIS 8L, ILS 9R, VIS 10.   \n",
      "29  SIMULTANEOUS APCHS IN USE VIS 8L, ILS 9R, VIS 10.   \n",
      "30  SIMULTANEOUS APCHS IN USE VIS 8L, ILS 9R, VIS 10.   \n",
      "31  SIMULTANEOUS APCHS IN USE VIS 8L, ILS 9R, VIS 10.   \n",
      "32  SIMULTANEOUS APCHS IN USE VIS 8L, ILS 9R, VIS 10.   \n",
      "33  SIMULTANEOUS APCHS IN USE ILS 8L, ILS 9R, VIS 10.   \n",
      "\n",
      "   airport_configuration_name file_type  \n",
      "0      D_26L_27R_A_26R_27L_28   configs  \n",
      "1      D_26L_27R_A_26R_27L_28   configs  \n",
      "2      D_26L_27R_A_26R_27L_28   configs  \n",
      "3      D_26L_27R_A_26R_27L_28   configs  \n",
      "4         D_26L_27R_A_26R_27R   configs  \n",
      "5         D_26L_27R_A_26R_27R   configs  \n",
      "6         D_26L_27R_A_26R_27R   configs  \n",
      "7         D_26L_27R_A_26R_27R   configs  \n",
      "8         D_26L_27R_A_26R_27R   configs  \n",
      "9         D_26L_27R_A_26R_27R   configs  \n",
      "10        D_26L_27R_A_26R_27R   configs  \n",
      "11        D_26L_27R_A_26R_27R   configs  \n",
      "12        D_26L_27R_A_26R_27R   configs  \n",
      "13        D_26L_27R_A_26R_27R   configs  \n",
      "14     D_26L_27R_A_26R_27L_28   configs  \n",
      "15     D_26L_27R_A_26R_27L_28   configs  \n",
      "16     D_26L_27R_A_26R_27L_28   configs  \n",
      "17         D_8R_9L_A_10_8L_9R   configs  \n",
      "18         D_8R_9L_A_10_8L_9R   configs  \n",
      "19         D_8R_9L_A_10_8L_9R   configs  \n",
      "20         D_8R_9L_A_10_8L_9R   configs  \n",
      "21         D_8R_9L_A_10_8L_9R   configs  \n",
      "22         D_8R_9L_A_10_8L_9R   configs  \n",
      "23         D_8R_9L_A_10_8L_9R   configs  \n",
      "24         D_8R_9L_A_10_8L_9R   configs  \n",
      "25         D_8R_9L_A_10_8L_9R   configs  \n",
      "26         D_8R_9L_A_10_8L_9R   configs  \n",
      "27         D_8R_9L_A_10_8L_9R   configs  \n",
      "28         D_8R_9L_A_10_8L_9R   configs  \n",
      "29         D_8R_9L_A_10_8L_9R   configs  \n",
      "30         D_8R_9L_A_10_8L_9R   configs  \n",
      "31         D_8R_9L_A_10_8L_9R   configs  \n",
      "32         D_8R_9L_A_10_8L_9R   configs  \n",
      "33         D_8R_9L_A_10_8L_9R   configs  \n",
      "./data/FUSER/FUSER_example.csv saved!\n"
     ]
    }
   ],
   "source": [
    "print(fuser_data)\n",
    "file_path = os.path.join('./', 'data', 'FUSER', f'FUSER_example.csv')\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "fuser_data.to_csv(file_path, index=False)\n",
    "print(file_path, \"saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metar_data = fetchData.load_data(\n",
    "    data_type=\"METAR\", \n",
    "    dataset_purpose=\"train\",  \n",
    "    path_level=\"part_1\",\n",
    "    file_name=\"metar.20220901.00Z\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      station datetime   cor   auto weather  temperature  dewpoint  \\\n",
      "0        AGGH  010000Z  None  False     SCT          NaN       NaN   \n",
      "1        AGGH  010000Z  None  False     SCT          NaN       NaN   \n",
      "2        AGGH  010000Z  None  False     SCT          NaN       NaN   \n",
      "3        AGGH  010000Z  None  False     SCT          NaN       NaN   \n",
      "4        AYMH  010000Z  None  False     HZ          20.0      17.0   \n",
      "...       ...      ...   ...    ...     ...          ...       ...   \n",
      "33587    ZYTX  010000Z  None  False    None         18.0       9.0   \n",
      "33588    ZYTX  010000Z  None  False    None         18.0       9.0   \n",
      "33589    ZYTX  010030Z  None  False    None         18.0       9.0   \n",
      "33590    ZYTX  010030Z  None  False    None         18.0       9.0   \n",
      "33591    ZYTX  010030Z  None  False    None         18.0       9.0   \n",
      "\n",
      "                      date_time  \\\n",
      "0     2022-09-01 00:00:00+00:00   \n",
      "1     2022-09-01 00:00:00+00:00   \n",
      "2     2022-09-01 00:00:00+00:00   \n",
      "3     2022-09-01 00:00:00+00:00   \n",
      "4     2022-09-01 00:00:00+00:00   \n",
      "...                         ...   \n",
      "33587 2022-09-01 00:00:00+00:00   \n",
      "33588 2022-09-01 00:00:00+00:00   \n",
      "33589 2022-09-01 00:30:00+00:00   \n",
      "33590 2022-09-01 00:30:00+00:00   \n",
      "33591 2022-09-01 00:30:00+00:00   \n",
      "\n",
      "                                            cloud_layers  visibility_meters  \\\n",
      "0                                                     []             9999.0   \n",
      "1                                                     []             9999.0   \n",
      "2                                                     []             9999.0   \n",
      "3                                                     []             9999.0   \n",
      "4      [{'sky_cover': 3, 'altitude_ft': 3000, 'cumulo...             9999.0   \n",
      "...                                                  ...                ...   \n",
      "33587                                                 []            10000.0   \n",
      "33588                                                 []            10000.0   \n",
      "33589                                                 []            10000.0   \n",
      "33590                                                 []            10000.0   \n",
      "33591                                                 []            10000.0   \n",
      "\n",
      "       wind_speed_mps  pressure  \n",
      "0                4.63       NaN  \n",
      "1                4.63       NaN  \n",
      "2                4.63       NaN  \n",
      "3                4.63       NaN  \n",
      "4                2.06    1022.0  \n",
      "...               ...       ...  \n",
      "33587            3.00    1020.0  \n",
      "33588            3.00    1020.0  \n",
      "33589            3.00    1020.0  \n",
      "33590            3.00    1020.0  \n",
      "33591            3.00    1020.0  \n",
      "\n",
      "[33592 rows x 12 columns]\n",
      "./data/METAR/METAR_example.csv saved!\n"
     ]
    }
   ],
   "source": [
    "print(metar_data)\n",
    "file_path = os.path.join('./', 'data', 'METAR', f'METAR_example.csv')\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "metar_data.to_csv(file_path, index=False)\n",
    "print(file_path, \"saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "taf_data = fetchData.load_data( \n",
    "    data_type=\"TAF\", \n",
    "    dataset_purpose=\"train\",  \n",
    "    file_name=\"taf.20220901.00Z\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      amended  corection station issue_datetime weather  max_temp_value  \\\n",
      "0       False      False    ZGKL       012104Z     None            32.0   \n",
      "1       False      False    ZGNN       012108Z     None            35.0   \n",
      "2       False      False    ZGOW       012101Z     None            35.0   \n",
      "3       False      False    ZGSZ       012120Z     None            34.0   \n",
      "4       False      False    ZLXY       012107Z       BR             NaN   \n",
      "...       ...        ...     ...            ...     ...             ...   \n",
      "1076    False      False    TLPL       012300Z     None             NaN   \n",
      "1077    False      False    TLPC       012300Z     None             NaN   \n",
      "1078    False      False    KNBG           None    None             NaN   \n",
      "1079    False      False    KNHK           None     SKC             NaN   \n",
      "1080    False      False    KNQX           None    None             NaN   \n",
      "\n",
      "      min_temp_value  probability  \\\n",
      "0               20.0          0.9   \n",
      "1               26.0          0.9   \n",
      "2               25.0          0.9   \n",
      "3               28.0          0.9   \n",
      "4                NaN          0.9   \n",
      "...              ...          ...   \n",
      "1076             NaN          0.3   \n",
      "1077             NaN          0.3   \n",
      "1078             NaN          0.9   \n",
      "1079             NaN          0.9   \n",
      "1080             NaN          0.9   \n",
      "\n",
      "                                    additional_sections  \\\n",
      "0                                                         \n",
      "1                                                         \n",
      "2                                                         \n",
      "3                                                         \n",
      "4                             NSC TX28/0208Z TN19/0222Z   \n",
      "...                                                 ...   \n",
      "1076                 TEMPO 0209/0218 5000 SHRA FEW015CB   \n",
      "1077                 TEMPO 0209/0218 5000 SHRA FEW015CB   \n",
      "1078  FM020300 VRB03KT 4800 BR FEW035 QNH2990INS FM0...   \n",
      "1079  BECMG 0214/0216 11007KT FEW060 FEW110 SCT280 Q...   \n",
      "1080  FM020700 11010KT VCSH QNK2995INS AUTOMATED SEN...   \n",
      "\n",
      "                     date_time  ... wind_gust_kt visibility_meters  \\\n",
      "0    2022-09-01 00:00:00+00:00  ...          NaN            7000.0   \n",
      "1    2022-09-01 00:00:00+00:00  ...          NaN            7000.0   \n",
      "2    2022-09-01 00:00:00+00:00  ...          NaN            6000.0   \n",
      "3    2022-09-01 00:00:00+00:00  ...          NaN            6000.0   \n",
      "4    2022-09-01 00:00:00+00:00  ...          NaN            3000.0   \n",
      "...                        ...  ...          ...               ...   \n",
      "1076 2022-09-01 00:07:00+00:00  ...          NaN            9999.0   \n",
      "1077 2022-09-01 00:07:00+00:00  ...          NaN            9999.0   \n",
      "1078 2022-09-01 23:08:00+00:00  ...          NaN            9999.0   \n",
      "1079 2022-09-01 23:08:00+00:00  ...          NaN            9999.0   \n",
      "1080 2022-09-01 23:08:00+00:00  ...          NaN            9999.0   \n",
      "\n",
      "      weather_score                                       cloud_layers  \\\n",
      "0               0.0  [{'sky_cover': 3, 'altitude_ft': 5000, 'cumulo...   \n",
      "1               0.0  [{'sky_cover': 3, 'altitude_ft': 3300, 'cumulo...   \n",
      "2               0.0  [{'sky_cover': 1, 'altitude_ft': 2000, 'cumulo...   \n",
      "3               0.0  [{'sky_cover': 3, 'altitude_ft': 2600, 'cumulo...   \n",
      "4               1.0                                                 []   \n",
      "...             ...                                                ...   \n",
      "1076            0.0  [{'sky_cover': 3, 'altitude_ft': 2500, 'cumulo...   \n",
      "1077            0.0  [{'sky_cover': 3, 'altitude_ft': 2500, 'cumulo...   \n",
      "1078            0.0  [{'sky_cover': 3, 'altitude_ft': 4000, 'cumulo...   \n",
      "1079            0.0                                                 []   \n",
      "1080            0.0  [{'sky_cover': 1, 'altitude_ft': 2500, 'cumulo...   \n",
      "\n",
      "       qnh_hpa  variable_wind_from  variable_wind_to  \\\n",
      "0          NaN                 NaN               NaN   \n",
      "1          NaN                 NaN               NaN   \n",
      "2          NaN                 NaN               NaN   \n",
      "3          NaN                 NaN               NaN   \n",
      "4          NaN                 NaN               NaN   \n",
      "...        ...                 ...               ...   \n",
      "1076       NaN                 NaN               NaN   \n",
      "1077       NaN                 NaN               NaN   \n",
      "1078  101084.0                 NaN               NaN   \n",
      "1079  101389.0                 NaN               NaN   \n",
      "1080  101389.0                 NaN               NaN   \n",
      "\n",
      "          max_temperature_time      min_temperature_time has_prob  \n",
      "0    2022-09-02 08:00:00+00:00 2022-09-02 22:00:00+00:00    False  \n",
      "1    2022-09-02 07:00:00+00:00 2022-09-02 22:00:00+00:00    False  \n",
      "2    2022-09-02 06:00:00+00:00 2022-09-02 22:00:00+00:00    False  \n",
      "3    2022-09-02 06:00:00+00:00 2022-09-02 22:00:00+00:00    False  \n",
      "4                          NaT                       NaT    False  \n",
      "...                        ...                       ...      ...  \n",
      "1076                       NaT                       NaT     True  \n",
      "1077                       NaT                       NaT     True  \n",
      "1078                       NaT                       NaT    False  \n",
      "1079                       NaT                       NaT    False  \n",
      "1080                       NaT                       NaT    False  \n",
      "\n",
      "[1081 rows x 26 columns]\n",
      "./data/TAF/TAF_example.csv saved!\n"
     ]
    }
   ],
   "source": [
    "print(taf_data)\n",
    "file_path = os.path.join('./', 'data', 'TAF', f'TAF_example.csv')\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "taf_data.to_csv(file_path, index=False)\n",
    "print(file_path, \"saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
