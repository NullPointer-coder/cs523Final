{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNrrZIR5dn-v",
        "outputId": "83e05944-adef-4711-e354-378a05a6117e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading data...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "No objects to concatenate",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 101\u001b[0m\n\u001b[0;32m     99\u001b[0m train_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrive/MyDrive/Colab Notebooks/dataset/FUSER/train\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrive/MyDrive/Colab Notebooks/dataset/FUSER/test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 101\u001b[0m \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m prepare_dataset(data_path\u001b[38;5;241m=\u001b[39mtest_path, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_data\u001b[39m\u001b[38;5;124m'\u001b[39m, output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[2], line 56\u001b[0m, in \u001b[0;36mprepare_dataset\u001b[1;34m(data_path, output_dir, output_file)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_dataset\u001b[39m(data_path, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_data\u001b[39m\u001b[38;5;124m'\u001b[39m, output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# load and merge data\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_merge_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# select specified features\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     weather_features \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwind_speed\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlightning_prob_N\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     76\u001b[0m     ]\n",
            "Cell \u001b[1;32mIn[2], line 10\u001b[0m, in \u001b[0;36mload_and_merge_data\u001b[1;34m(train_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file)\n\u001b[0;32m      8\u001b[0m     df_list\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m---> 10\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m merged_df\n",
            "File \u001b[1;32mc:\\Users\\11923\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\11923\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[0;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
            "File \u001b[1;32mc:\\Users\\11923\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
            "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# load and merge data\n",
        "def load_and_merge_data(train_path):\n",
        "    all_files = glob.glob(str(Path(train_path) / \"*.csv\"))\n",
        "    df_list = []\n",
        "\n",
        "    for file in all_files:\n",
        "        df = pd.read_csv(file)\n",
        "        df_list.append(df)\n",
        "\n",
        "    merged_df = pd.concat(df_list, ignore_index=True)\n",
        "    return merged_df\n",
        "\n",
        "# handle missing values\n",
        "def handle_missing_values(df):\n",
        "    # numeric features\n",
        "    numeric_features = [\n",
        "        'temperature',\n",
        "        'wind_speed',\n",
        "        'visibility',\n",
        "        'cloud_ceiling',\n",
        "        'wind_direction',\n",
        "        'wind_gust',\n",
        "        'precip',\n",
        "        'arrival_count'\n",
        "    ]\n",
        "\n",
        "    # boolean features\n",
        "    boolean_features = [\n",
        "        'cloud_BK',\n",
        "        'cloud_CL',\n",
        "        'cloud_FW',\n",
        "        'cloud_OV',\n",
        "        'cloud_SC',\n",
        "        'lightning_prob_H',\n",
        "        'lightning_prob_L',\n",
        "        'lightning_prob_M',\n",
        "        'lightning_prob_N'\n",
        "    ]\n",
        "\n",
        "    # fill numeric features with median\n",
        "    for feature in numeric_features:\n",
        "        if feature in df.columns:\n",
        "            df[feature] = df[feature].fillna(df[feature].median())\n",
        "\n",
        "    # fill boolean features\n",
        "    for feature in boolean_features:\n",
        "        if feature in df.columns:\n",
        "            df[feature] = df[feature].fillna(False)\n",
        "\n",
        "    return df\n",
        "\n",
        "# prepare dataset\n",
        "def prepare_dataset(data_path, output_dir='processed_data', output_file='processed_train.csv'):\n",
        "    # load and merge data\n",
        "    print(\"loading data...\")\n",
        "    df = load_and_merge_data(data_path)\n",
        "\n",
        "    # select specified features\n",
        "    weather_features = [\n",
        "        'temperature',\n",
        "        'wind_speed',\n",
        "        'visibility',\n",
        "        'cloud_ceiling',\n",
        "        'wind_direction',\n",
        "        'wind_gust',\n",
        "        'precip',\n",
        "        'cloud_BK',\n",
        "        'cloud_CL',\n",
        "        'cloud_FW',\n",
        "        'cloud_OV',\n",
        "        'cloud_SC',\n",
        "        'lightning_prob_H',\n",
        "        'lightning_prob_L',\n",
        "        'lightning_prob_M',\n",
        "        'lightning_prob_N'\n",
        "    ]\n",
        "\n",
        "    # prepare final dataset\n",
        "    X = df[['timestamp_15mins', 'airport_id'] + weather_features]\n",
        "    y = df['arrival_count']\n",
        "\n",
        "    # create output dir\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # combine features and labels into dataframe\n",
        "    data = pd.concat([X, pd.Series(y, name='arrival_count')], axis=1)\n",
        "\n",
        "    # handle missing values\n",
        "    data = handle_missing_values(data)\n",
        "\n",
        "    # save processed data\n",
        "    data.to_csv(output_path / output_file, index=False)\n",
        "    print(f\"\\nprocessed data:\")\n",
        "    print(f\"shape: {data.shape}\")\n",
        "    print(f\"columns: {list(data.columns)}\")\n",
        "    print(f\"\\ndata saved to: {output_path / output_file}\")\n",
        "\n",
        "train_path = \"drive/MyDrive/Colab Notebooks/dataset/FUSER/train\"\n",
        "test_path = \"drive/MyDrive/Colab Notebooks/dataset/FUSER/test\"\n",
        "prepare_dataset(data_path=train_path, output_dir='processed_data', output_file='processed_train.csv')\n",
        "prepare_dataset(data_path=test_path, output_dir='processed_data', output_file='processed_test.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "-zVFQ1-ZdrS2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AirportNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(AirportNet, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            #nn.BatchNorm1d(128),\n",
        "            #nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            #nn.BatchNorm1d(64),\n",
        "            #nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            #nn.BatchNorm1d(32),\n",
        "            #nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.kaiming_normal_(m.weight)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "CRUi2haUeflS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class AirportDataset(Dataset):\n",
        "    def __init__(self, data_path, feature_scaler=None, target_scaler=None, is_train=True):\n",
        "        # read data with dtype specification\n",
        "        dtype_dict = {\n",
        "            # 'precip': 'float64',\n",
        "            'timestamp_15mins': str,\n",
        "            'airport_id': str,\n",
        "            'arrival_count': 'float64'\n",
        "        }\n",
        "        self.df = pd.read_csv(data_path, dtype=dtype_dict)\n",
        "\n",
        "        self.df['precip'] = pd.to_numeric(self.df['precip'], errors='coerce')\n",
        "        self.df['precip'] = self.df['precip'].fillna(0)\n",
        "\n",
        "        # separate features and target\n",
        "        feature_cols = [col for col in self.df.columns if col not in ['timestamp_15mins', 'airport_id', 'arrival_count']]\n",
        "\n",
        "        # convert bool to int\n",
        "        for col in feature_cols:\n",
        "            if self.df[col].dtype == bool:\n",
        "                self.df[col] = self.df[col].astype(int)\n",
        "            elif self.df[col].dtype == object:\n",
        "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
        "\n",
        "        # ensure precip column is properly handled\n",
        "        self.df['precip'] = pd.to_numeric(self.df['precip'], errors='coerce')\n",
        "        self.df['precip'] = self.df['precip'].fillna(0)\n",
        "\n",
        "        # feature standardization\n",
        "        if is_train:\n",
        "            self.feature_scaler = StandardScaler()\n",
        "            features_normalized = self.feature_scaler.fit_transform(self.df[feature_cols])\n",
        "            self.target_scaler = StandardScaler()\n",
        "            targets_normalized = self.target_scaler.fit_transform(self.df[['arrival_count']])\n",
        "        else:\n",
        "            self.feature_scaler = feature_scaler\n",
        "            self.target_scaler = target_scaler\n",
        "            features_normalized = self.feature_scaler.transform(self.df[feature_cols])\n",
        "            targets_normalized = self.target_scaler.transform(self.df[['arrival_count']])\n",
        "\n",
        "        # convert to tensor\n",
        "        self.X = torch.FloatTensor(features_normalized)\n",
        "        self.y = torch.FloatTensor(targets_normalized.squeeze())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# create data loaders\n",
        "def get_data_loaders(train_path, test_path, batch_size=64, num_workers=0, val_ratio=0.2):\n",
        "    # create train set and get scalers\n",
        "    full_train_dataset = AirportDataset(train_path, is_train=True)\n",
        "    feature_scaler = full_train_dataset.feature_scaler\n",
        "    target_scaler = full_train_dataset.target_scaler\n",
        "\n",
        "    # create test set using train scalers\n",
        "    test_dataset = AirportDataset(\n",
        "        test_path,\n",
        "        feature_scaler=feature_scaler,\n",
        "        target_scaler=target_scaler,\n",
        "        is_train=False\n",
        "    )\n",
        "\n",
        "    # calculate sizes for train and val sets\n",
        "    val_size = int(len(full_train_dataset) * val_ratio)\n",
        "    train_size = len(full_train_dataset) - val_size\n",
        "\n",
        "    # split train set into train and val\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        full_train_dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "\n",
        "    # create data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataset Info:\")\n",
        "    print(f\"Training samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"Testing samples: {len(test_dataset)}\")\n",
        "    print(f\"Feature dimension: {full_train_dataset.X.shape[1]}\")\n",
        "    return train_loader, val_loader, test_loader, target_scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4sFzMVAe6pw",
        "outputId": "c30a4b85-c991-42a0-e920-8c7de684aa78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-33-d2d60de09934>:15: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  self.df = pd.read_csv(data_path, dtype=dtype_dict)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset Info:\n",
            "Training samples: 216504\n",
            "Validation samples: 54126\n",
            "Testing samples: 85360\n",
            "Feature dimension: 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/500] - 3.15s\n",
            "Train RMSE: 1.0020, Val RMSE: 0.9833\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9833\n",
            "\n",
            "Epoch [2/500] - 2.94s\n",
            "Train RMSE: 0.9941, Val RMSE: 0.9826\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9826\n",
            "\n",
            "Epoch [3/500] - 2.92s\n",
            "Train RMSE: 0.9916, Val RMSE: 0.9805\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9805\n",
            "\n",
            "Epoch [4/500] - 3.12s\n",
            "Train RMSE: 0.9892, Val RMSE: 0.9771\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9771\n",
            "\n",
            "Epoch [5/500] - 2.97s\n",
            "Train RMSE: 0.9873, Val RMSE: 0.9758\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9758\n",
            "\n",
            "Epoch [6/500] - 2.93s\n",
            "Train RMSE: 0.9856, Val RMSE: 0.9732\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9732\n",
            "\n",
            "Epoch [7/500] - 2.92s\n",
            "Train RMSE: 0.9829, Val RMSE: 0.9721\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9721\n",
            "\n",
            "Epoch [8/500] - 3.12s\n",
            "Train RMSE: 0.9815, Val RMSE: 0.9710\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9710\n",
            "\n",
            "Epoch [9/500] - 2.95s\n",
            "Train RMSE: 0.9800, Val RMSE: 0.9699\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9699\n",
            "\n",
            "Epoch [10/500] - 2.96s\n",
            "Train RMSE: 0.9784, Val RMSE: 0.9673\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9673\n",
            "\n",
            "Epoch [11/500] - 3.09s\n",
            "Train RMSE: 0.9766, Val RMSE: 0.9662\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9662\n",
            "\n",
            "Epoch [12/500] - 2.97s\n",
            "Train RMSE: 0.9751, Val RMSE: 0.9664\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [13/500] - 3.04s\n",
            "Train RMSE: 0.9735, Val RMSE: 0.9668\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [14/500] - 3.10s\n",
            "Train RMSE: 0.9722, Val RMSE: 0.9623\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9623\n",
            "\n",
            "Epoch [15/500] - 2.93s\n",
            "Train RMSE: 0.9709, Val RMSE: 0.9633\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [16/500] - 2.92s\n",
            "Train RMSE: 0.9698, Val RMSE: 0.9609\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9609\n",
            "\n",
            "Epoch [17/500] - 3.10s\n",
            "Train RMSE: 0.9684, Val RMSE: 0.9595\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9595\n",
            "\n",
            "Epoch [18/500] - 2.95s\n",
            "Train RMSE: 0.9676, Val RMSE: 0.9596\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [19/500] - 2.92s\n",
            "Train RMSE: 0.9662, Val RMSE: 0.9587\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9587\n",
            "\n",
            "Epoch [20/500] - 3.10s\n",
            "Train RMSE: 0.9652, Val RMSE: 0.9582\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9582\n",
            "\n",
            "Epoch [21/500] - 2.94s\n",
            "Train RMSE: 0.9644, Val RMSE: 0.9571\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9571\n",
            "\n",
            "Epoch [22/500] - 2.98s\n",
            "Train RMSE: 0.9631, Val RMSE: 0.9555\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9555\n",
            "\n",
            "Epoch [23/500] - 3.08s\n",
            "Train RMSE: 0.9623, Val RMSE: 0.9558\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [24/500] - 2.95s\n",
            "Train RMSE: 0.9615, Val RMSE: 0.9552\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9552\n",
            "\n",
            "Epoch [25/500] - 2.93s\n",
            "Train RMSE: 0.9602, Val RMSE: 0.9533\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9533\n",
            "\n",
            "Epoch [26/500] - 3.14s\n",
            "Train RMSE: 0.9595, Val RMSE: 0.9534\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [27/500] - 2.95s\n",
            "Train RMSE: 0.9588, Val RMSE: 0.9526\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9526\n",
            "\n",
            "Epoch [28/500] - 2.95s\n",
            "Train RMSE: 0.9584, Val RMSE: 0.9541\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [29/500] - 3.11s\n",
            "Train RMSE: 0.9573, Val RMSE: 0.9523\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9523\n",
            "\n",
            "Epoch [30/500] - 2.96s\n",
            "Train RMSE: 0.9567, Val RMSE: 0.9522\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9522\n",
            "\n",
            "Epoch [31/500] - 2.93s\n",
            "Train RMSE: 0.9561, Val RMSE: 0.9519\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9519\n",
            "\n",
            "Epoch [32/500] - 3.08s\n",
            "Train RMSE: 0.9555, Val RMSE: 0.9502\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9502\n",
            "\n",
            "Epoch [33/500] - 2.94s\n",
            "Train RMSE: 0.9548, Val RMSE: 0.9501\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9501\n",
            "\n",
            "Epoch [34/500] - 2.98s\n",
            "Train RMSE: 0.9540, Val RMSE: 0.9487\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9487\n",
            "\n",
            "Epoch [35/500] - 3.09s\n",
            "Train RMSE: 0.9541, Val RMSE: 0.9490\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [36/500] - 2.93s\n",
            "Train RMSE: 0.9526, Val RMSE: 0.9494\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [37/500] - 2.92s\n",
            "Train RMSE: 0.9519, Val RMSE: 0.9493\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [38/500] - 3.11s\n",
            "Train RMSE: 0.9514, Val RMSE: 0.9506\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [39/500] - 2.97s\n",
            "Train RMSE: 0.9516, Val RMSE: 0.9496\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [40/500] - 2.93s\n",
            "Train RMSE: 0.9505, Val RMSE: 0.9476\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9476\n",
            "\n",
            "Epoch [41/500] - 3.07s\n",
            "Train RMSE: 0.9498, Val RMSE: 0.9471\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9471\n",
            "\n",
            "Epoch [42/500] - 2.93s\n",
            "Train RMSE: 0.9497, Val RMSE: 0.9459\n",
            "Learning Rate: 0.001000\n",
            "Saved best model with val_loss: 0.9459\n",
            "\n",
            "Epoch [43/500] - 2.99s\n",
            "Train RMSE: 0.9493, Val RMSE: 0.9468\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [44/500] - 3.06s\n",
            "Train RMSE: 0.9489, Val RMSE: 0.9475\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [45/500] - 2.93s\n",
            "Train RMSE: 0.9481, Val RMSE: 0.9478\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [46/500] - 2.94s\n",
            "Train RMSE: 0.9478, Val RMSE: 0.9468\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [47/500] - 3.16s\n",
            "Train RMSE: 0.9477, Val RMSE: 0.9462\n",
            "Learning Rate: 0.001000\n",
            "\n",
            "Epoch [48/500] - 2.93s\n",
            "Train RMSE: 0.9461, Val RMSE: 0.9462\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [49/500] - 2.94s\n",
            "Train RMSE: 0.9411, Val RMSE: 0.9420\n",
            "Learning Rate: 0.000100\n",
            "Saved best model with val_loss: 0.9420\n",
            "\n",
            "Epoch [50/500] - 3.09s\n",
            "Train RMSE: 0.9380, Val RMSE: 0.9416\n",
            "Learning Rate: 0.000100\n",
            "Saved best model with val_loss: 0.9416\n",
            "\n",
            "Epoch [51/500] - 2.93s\n",
            "Train RMSE: 0.9367, Val RMSE: 0.9411\n",
            "Learning Rate: 0.000100\n",
            "Saved best model with val_loss: 0.9411\n",
            "\n",
            "Epoch [52/500] - 2.94s\n",
            "Train RMSE: 0.9374, Val RMSE: 0.9411\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [53/500] - 3.10s\n",
            "Train RMSE: 0.9368, Val RMSE: 0.9407\n",
            "Learning Rate: 0.000100\n",
            "Saved best model with val_loss: 0.9407\n",
            "\n",
            "Epoch [54/500] - 2.99s\n",
            "Train RMSE: 0.9368, Val RMSE: 0.9406\n",
            "Learning Rate: 0.000100\n",
            "Saved best model with val_loss: 0.9406\n",
            "\n",
            "Epoch [55/500] - 2.92s\n",
            "Train RMSE: 0.9372, Val RMSE: 0.9399\n",
            "Learning Rate: 0.000100\n",
            "Saved best model with val_loss: 0.9399\n",
            "\n",
            "Epoch [56/500] - 3.09s\n",
            "Train RMSE: 0.9364, Val RMSE: 0.9404\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [57/500] - 2.92s\n",
            "Train RMSE: 0.9354, Val RMSE: 0.9403\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [58/500] - 2.93s\n",
            "Train RMSE: 0.9361, Val RMSE: 0.9400\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [59/500] - 3.09s\n",
            "Train RMSE: 0.9354, Val RMSE: 0.9398\n",
            "Learning Rate: 0.000100\n",
            "Saved best model with val_loss: 0.9398\n",
            "\n",
            "Epoch [60/500] - 2.96s\n",
            "Train RMSE: 0.9352, Val RMSE: 0.9398\n",
            "Learning Rate: 0.000100\n",
            "Saved best model with val_loss: 0.9398\n",
            "\n",
            "Epoch [61/500] - 2.93s\n",
            "Train RMSE: 0.9350, Val RMSE: 0.9395\n",
            "Learning Rate: 0.000100\n",
            "Saved best model with val_loss: 0.9395\n",
            "\n",
            "Epoch [62/500] - 3.10s\n",
            "Train RMSE: 0.9352, Val RMSE: 0.9399\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [63/500] - 2.95s\n",
            "Train RMSE: 0.9354, Val RMSE: 0.9394\n",
            "Learning Rate: 0.000100\n",
            "Saved best model with val_loss: 0.9394\n",
            "\n",
            "Epoch [64/500] - 2.95s\n",
            "Train RMSE: 0.9350, Val RMSE: 0.9396\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [65/500] - 3.07s\n",
            "Train RMSE: 0.9352, Val RMSE: 0.9395\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [66/500] - 2.91s\n",
            "Train RMSE: 0.9343, Val RMSE: 0.9393\n",
            "Learning Rate: 0.000100\n",
            "Saved best model with val_loss: 0.9393\n",
            "\n",
            "Epoch [67/500] - 2.92s\n",
            "Train RMSE: 0.9344, Val RMSE: 0.9396\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [68/500] - 3.10s\n",
            "Train RMSE: 0.9347, Val RMSE: 0.9387\n",
            "Learning Rate: 0.000100\n",
            "Saved best model with val_loss: 0.9387\n",
            "\n",
            "Epoch [69/500] - 2.94s\n",
            "Train RMSE: 0.9343, Val RMSE: 0.9389\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [70/500] - 2.93s\n",
            "Train RMSE: 0.9339, Val RMSE: 0.9389\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [71/500] - 3.07s\n",
            "Train RMSE: 0.9340, Val RMSE: 0.9388\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [72/500] - 2.97s\n",
            "Train RMSE: 0.9341, Val RMSE: 0.9391\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [73/500] - 2.95s\n",
            "Train RMSE: 0.9346, Val RMSE: 0.9391\n",
            "Learning Rate: 0.000100\n",
            "\n",
            "Epoch [74/500] - 2.94s\n",
            "Train RMSE: 0.9340, Val RMSE: 0.9390\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [75/500] - 3.15s\n",
            "Train RMSE: 0.9330, Val RMSE: 0.9387\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [76/500] - 2.94s\n",
            "Train RMSE: 0.9323, Val RMSE: 0.9386\n",
            "Learning Rate: 0.000010\n",
            "Saved best model with val_loss: 0.9386\n",
            "\n",
            "Epoch [77/500] - 2.96s\n",
            "Train RMSE: 0.9334, Val RMSE: 0.9386\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [78/500] - 3.08s\n",
            "Train RMSE: 0.9322, Val RMSE: 0.9386\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [79/500] - 2.93s\n",
            "Train RMSE: 0.9321, Val RMSE: 0.9387\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [80/500] - 2.94s\n",
            "Train RMSE: 0.9326, Val RMSE: 0.9385\n",
            "Learning Rate: 0.000010\n",
            "Saved best model with val_loss: 0.9385\n",
            "\n",
            "Epoch [81/500] - 2.99s\n",
            "Train RMSE: 0.9317, Val RMSE: 0.9386\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [82/500] - 3.08s\n",
            "Train RMSE: 0.9324, Val RMSE: 0.9386\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [83/500] - 2.96s\n",
            "Train RMSE: 0.9328, Val RMSE: 0.9386\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [84/500] - 2.94s\n",
            "Train RMSE: 0.9329, Val RMSE: 0.9386\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [85/500] - 3.14s\n",
            "Train RMSE: 0.9325, Val RMSE: 0.9384\n",
            "Learning Rate: 0.000010\n",
            "Saved best model with val_loss: 0.9384\n",
            "\n",
            "Epoch [86/500] - 2.93s\n",
            "Train RMSE: 0.9323, Val RMSE: 0.9384\n",
            "Learning Rate: 0.000010\n",
            "Saved best model with val_loss: 0.9384\n",
            "\n",
            "Epoch [87/500] - 2.96s\n",
            "Train RMSE: 0.9328, Val RMSE: 0.9384\n",
            "Learning Rate: 0.000010\n",
            "Saved best model with val_loss: 0.9384\n",
            "\n",
            "Epoch [88/500] - 3.09s\n",
            "Train RMSE: 0.9324, Val RMSE: 0.9385\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [89/500] - 2.95s\n",
            "Train RMSE: 0.9327, Val RMSE: 0.9385\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [90/500] - 2.92s\n",
            "Train RMSE: 0.9323, Val RMSE: 0.9385\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [91/500] - 3.11s\n",
            "Train RMSE: 0.9323, Val RMSE: 0.9386\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [92/500] - 2.91s\n",
            "Train RMSE: 0.9324, Val RMSE: 0.9386\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [93/500] - 2.95s\n",
            "Train RMSE: 0.9319, Val RMSE: 0.9383\n",
            "Learning Rate: 0.000010\n",
            "Saved best model with val_loss: 0.9383\n",
            "\n",
            "Epoch [94/500] - 3.10s\n",
            "Train RMSE: 0.9321, Val RMSE: 0.9386\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [95/500] - 3.00s\n",
            "Train RMSE: 0.9325, Val RMSE: 0.9385\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [96/500] - 2.92s\n",
            "Train RMSE: 0.9323, Val RMSE: 0.9385\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [97/500] - 3.10s\n",
            "Train RMSE: 0.9323, Val RMSE: 0.9386\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [98/500] - 2.93s\n",
            "Train RMSE: 0.9318, Val RMSE: 0.9386\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [99/500] - 2.95s\n",
            "Train RMSE: 0.9325, Val RMSE: 0.9385\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [100/500] - 3.08s\n",
            "Train RMSE: 0.9317, Val RMSE: 0.9384\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [101/500] - 2.92s\n",
            "Train RMSE: 0.9321, Val RMSE: 0.9384\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [102/500] - 2.98s\n",
            "Train RMSE: 0.9328, Val RMSE: 0.9385\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Epoch [103/500] - 3.11s\n",
            "Train RMSE: 0.9325, Val RMSE: 0.9383\n",
            "Learning Rate: 0.000010\n",
            "\n",
            "Early stopping after 10 epochs without improvement\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "\n",
        "    def forward(self, yhat, y):\n",
        "        return torch.sqrt(self.mse(yhat, y))\n",
        "\n",
        "# train the model\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                num_epochs=100, device='cuda', save_dir='checkpoints'):\n",
        "    save_path = Path(save_dir)\n",
        "    save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10  # early stopping patience\n",
        "    no_improve = 0  # epochs without improvement\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # training phase\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "\n",
        "            # gradient clipping to prevent explosion\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        # adjust learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # print training info\n",
        "        epoch_time = time.time() - start_time\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] - {epoch_time:.2f}s')\n",
        "        print(f'Train RMSE: {train_loss:.4f}, Val RMSE: {val_loss:.4f}')\n",
        "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "\n",
        "        # save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            no_improve = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "            }, save_path / 'best_model.pth')\n",
        "            print(f'Saved best model with val_loss: {val_loss:.4f}')\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        # early stopping check\n",
        "        if no_improve >= patience:\n",
        "            print(f'\\nEarly stopping after {patience} epochs without improvement')\n",
        "            break\n",
        "\n",
        "        print()\n",
        "\n",
        "# main training script\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    # load data\n",
        "    train_path = \"processed_data/processed_train.csv\"\n",
        "    test_path = \"processed_data/processed_test.csv\"\n",
        "    train_loader, val_loader, _, _ = get_data_loaders(\n",
        "        train_path,\n",
        "        test_path,\n",
        "        batch_size=512,\n",
        "        val_ratio=0.2\n",
        "    )\n",
        "\n",
        "    # create model\n",
        "    input_dim = train_loader.dataset.dataset.X.shape[1]\n",
        "    model = AirportNet(input_dim=input_dim).to(device)\n",
        "\n",
        "    # setup loss and optimizer\n",
        "    criterion = RMSELoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "    # add learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=0.1,\n",
        "        patience=5,\n",
        "        min_lr=0.00001,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    # train the model\n",
        "    train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        num_epochs=500,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhkIbbwOkutX",
        "outputId": "dbbb4347-fc12-4f21-f732-96b064029b74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-33-d2d60de09934>:15: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  self.df = pd.read_csv(data_path, dtype=dtype_dict)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset Info:\n",
            "Training samples: 216504\n",
            "Validation samples: 54126\n",
            "Testing samples: 85360\n",
            "Feature dimension: 16\n",
            "Loaded model from epoch 92 with validation loss: 0.9383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-67-73d0e14f070d>:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results:\n",
            "RMSE: 0.9679\n",
            "\n",
            "Results saved to result.csv\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# test model and generate submission format results\n",
        "def test_model(model, test_loader, target_scaler, device='cuda'):\n",
        "    model.eval()\n",
        "    all_outputs = []\n",
        "    all_targets = []\n",
        "    timestamps = []\n",
        "    airport_ids = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (batch_X, batch_y) in enumerate(test_loader):\n",
        "            # get original data\n",
        "            start_idx = i * test_loader.batch_size\n",
        "            end_idx = min((i + 1) * test_loader.batch_size, len(test_loader.dataset))\n",
        "\n",
        "            batch_timestamps = test_loader.dataset.df['timestamp_15mins'].values[start_idx:end_idx]\n",
        "            batch_airports = test_loader.dataset.df['airport_id'].values[start_idx:end_idx]\n",
        "\n",
        "            # get predictions (normalized space)\n",
        "            batch_X = batch_X.to(device)\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "            # collect predictions and targets in normalized space\n",
        "            all_outputs.append(outputs.cpu())\n",
        "            all_targets.append(batch_y)\n",
        "\n",
        "            # save timestamps and airport IDs\n",
        "            timestamps.extend(batch_timestamps)\n",
        "            airport_ids.extend(batch_airports)\n",
        "\n",
        "    # calculate RMSE\n",
        "    all_outputs = torch.cat(all_outputs, dim=0).numpy()\n",
        "    all_targets = torch.cat(all_targets, dim=0).numpy()\n",
        "\n",
        "    normalized_mse = np.mean((all_outputs - all_targets) ** 2)\n",
        "    normalized_rmse = np.sqrt(normalized_mse)\n",
        "\n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"RMSE: {normalized_rmse:.4f}\")\n",
        "\n",
        "    # convert predictions to original space for submission\n",
        "    predictions = target_scaler.inverse_transform(all_outputs.reshape(-1, 1))\n",
        "    predictions = predictions.squeeze()\n",
        "\n",
        "    # ensure all arrays have same length\n",
        "    assert len(predictions) == len(timestamps) == len(airport_ids), \\\n",
        "        f\"Length mismatch: predictions({len(predictions)}), timestamps({len(timestamps)}), airport_ids({len(airport_ids)})\"\n",
        "\n",
        "    # create submission format dataframe\n",
        "    results_df = pd.DataFrame({\n",
        "        'timestamp_15mins': timestamps,\n",
        "        'airport_id': airport_ids,\n",
        "        'prediction': predictions\n",
        "    })\n",
        "\n",
        "    # create id column\n",
        "    results_df['ID'] = results_df.apply(\n",
        "        lambda row: f\"{row['airport_id']}_{pd.to_datetime(row['timestamp_15mins']).strftime('%y%m%d_%H%M')}_15\",\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # prep final submission format\n",
        "    submission_df = pd.DataFrame({\n",
        "        'ID': results_df['ID'],\n",
        "        'Value': results_df['prediction'].round().clip(0)\n",
        "    })\n",
        "\n",
        "    # save results\n",
        "    submission_df.to_csv('result.csv', index=False)\n",
        "    print(\"\\nResults saved to result.csv\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    train_path = \"processed_data/processed_train.csv\"\n",
        "    test_path = \"processed_data/processed_test.csv\"\n",
        "    _, _, test_loader, target_scaler = get_data_loaders(train_path, test_path, batch_size=64)\n",
        "\n",
        "    checkpoint_path = Path('checkpoints/best_model.pth')\n",
        "\n",
        "    input_dim = test_loader.dataset.X.shape[1]\n",
        "    model = AirportNet(input_dim=input_dim).to(device)\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded model from epoch {checkpoint['epoch']} with validation loss: {checkpoint['val_loss']:.4f}\")\n",
        "\n",
        "    test_model(model, test_loader, target_scaler, device)\n",
        "\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
